{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b25663beab90ae83024dffde40993f1001b9a8604896fa4c67256a04dde7dc3b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "from sutime import SUTime\n",
    "sutime = SUTime(mark_time_ranges=True, include_range=True)\n",
    "\n",
    "import csv, sqlite3\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "query_file_path = \"../possible-questions.txt\"\n",
    "\n",
    "create_database_flag=False\n",
    "dataset_path=\"../dataset/covid-19/required_only/\"\n",
    "database_path=r\"..\\dataset\\covid-19\\mysql_database\\covid19.db\"\n",
    "\n",
    "parsed_parameter_save_path='../dataset/covid-19/parsed_parameters.pickle'\n",
    "\n",
    "additional_stopwords=['case', 'find', 'covid', 'coronavirus', 'covid-19', 'covid19', 'world']\n",
    "\n",
    "assign_base_words={\n",
    "    'recover' : ['recover','recovery','cure','heal'],\n",
    "    'death' : ['death','fatality','fatal','demise','decease','die','expire'],\n",
    "    'confirm': ['confirm'],\n",
    "    'active' : ['active', 'live'],\n",
    "    'maximum' : ['maximum', 'high', 'max', 'maximal', 'most'],\n",
    "    'minimum' : ['minimum', 'low', 'least', 'min'],\n",
    "    'average' : ['average', 'avg', 'normally', 'usually', 'generally'],\n",
    "    'state' : ['state', 'province'],\n",
    "    'country' : ['country', 'region', 'nation', 'place']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(query_file_path):\n",
    "    queries=[]\n",
    "    with open(query_file_path,\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line[:-1]\n",
    "            if(line):\n",
    "                queries.append(line)\n",
    "\n",
    "    return queries\n",
    "\n",
    "def print_entities(sentence):\n",
    "\tdoc = nlp(sentence)\n",
    "\tprint(\"----> Entities:\")\n",
    "\tfor ent in doc.ents: \n",
    "\t\tprint(\"-------->\",ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "def print_tokens(sentence):\n",
    "\tdoc = nlp(sentence) \n",
    "\tprint(\"----> Tokens:\")\n",
    "\tfor token in doc: \n",
    "\t\tprint(\"-------->\", token.text, token.pos_, token.dep_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = read_queries(query_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stopwords(additional_stopwords):\n",
    "    for word in additional_stopwords:\n",
    "        nlp.vocab[word].is_stop=True\n",
    "\n",
    "def get_reverse_dict(assign_base_words):\n",
    "    reverse_base_word_dict={}\n",
    "    for base, l in assign_base_words.items():\n",
    "        for item in l:\n",
    "            doc = nlp(item)\n",
    "            item = doc[0].lemma_\n",
    "            reverse_base_word_dict[item]=base\n",
    "\n",
    "    return reverse_base_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stopwords(additional_stopwords)\n",
    "reverse_base_word_dict = get_reverse_dict(assign_base_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_table(cur, path, csv_name, table_name):\n",
    "    with open(dataset_path+csv_name,'r') as fin:\n",
    "        dr = csv.DictReader(fin)\n",
    "        to_db=[tuple(i.values()) for i in dr]\n",
    "    \n",
    "    count=len(dr.fieldnames)\n",
    "    bindings=\"?, \"*count\n",
    "\n",
    "    cur.executemany(\"INSERT INTO \"+table_name+\" VALUES (\"+bindings[:-2]+\");\", to_db)\n",
    "\n",
    "def create_database():\n",
    "    print(\"Creating Database -> covid19.db\\n\")\n",
    "    con = sqlite3.connect(database_path)\n",
    "    cur = con.cursor()\n",
    "\n",
    "    tables=[(\"worldwide_aggregate\"),(\"reference\"),(\"timeseries\"),(\"us\"),(\"countries_aggregated\")]\n",
    "    for table in tables:\n",
    "        cur.execute(\"DROP TABLE IF EXISTS \"+table+\";\")\n",
    "\n",
    "    cur.execute(\"create table worldwide_aggregate(Date Date NOT NULL, Confirmed BIGINT NOT NULL, Recovered BIGINT NOT NULL, Deaths BIGINT NOT NULL, Increase_rate FLOAT default NULL, PRIMARY KEY (Date));\")\n",
    "    print(tables[0]+\" table created\\n    Loading Data ...\")\n",
    "    csv_to_table(cur,dataset_path,\"worldwide-aggregate.csv\", tables[0])\n",
    "    print(\"    Data Loading Complete\\n\")\n",
    "\n",
    "    cur.execute(\"create table us(Date Date NOT NULL, Admin2 VARCHAR(100) NOT NULL, Province_State VARCHAR(100) NOT NULL, Confirmed BIGINT NOT NULL, Deaths BIGINT NOT NULL, Country_Region VARCHAR(100) NOT NULL, PRIMARY KEY (Date, Admin2, Province_State));\")\n",
    "    print(tables[3]+\" table created\\n    Loading Data ...\")\n",
    "    csv_to_table(cur,dataset_path,\"us_simplified.csv\", tables[3])\n",
    "    print(\"    Data Loading Complete\\n\")\n",
    "\n",
    "    cur.execute(\"create table reference(UID INT NOT NULL, iso2 VARCHAR(20), iso3 VARCHAR(20), code3 INT, FIPS INT, Admin2 VARCHAR(100) NOT NULL, Province_State VARCHAR(100) NOT NULL, Country_Region VARCHAR(100) NOT NULL, Lat FLOAT NOT NULL, Long_ FLOAT NOT NULL, Combined_Key VARCHAR(100), Popolation BIGINT NOT NULL, PRIMARY KEY (UID));\")\n",
    "    print(tables[1]+\" table created\\n    Loading Data ...\")\n",
    "    csv_to_table(cur,dataset_path,\"reference.csv\", tables[1])\n",
    "    print(\"    Data Loading Complete\\n\")\n",
    "\n",
    "    cur.execute(\"create table timeseries(Date Date NOT NULL, Country_Region VARCHAR(100) NOT NULL, Province_State VARCHAR(100), Confirmed BIGINT NOT NULL, Recovered BIGINT NOT NULL, Deaths BIGINT NOT NULL, PRIMARY KEY (Date, Country_Region, Province_State));\")\n",
    "    print(tables[2]+\" table created\\n    Loading Data ...\")\n",
    "    csv_to_table(cur,dataset_path,\"time-series-19-covid-combined.csv\", tables[2])\n",
    "    print(\"    Data Loading Complete\\n\")\n",
    "\n",
    "    cur.execute(\"create table countries_aggregated(Date Date NOT NULL, Country VARCHAR(100) NOT NULL, Confirmed BIGINT NOT NULL, Recovered BIGINT NOT NULL, Deaths BIGINT NOT NULL, PRIMARY KEY (Date, Country));\")\n",
    "    print(tables[4]+\" table created\\n    Loading Data ...\")\n",
    "    csv_to_table(cur,dataset_path,\"countries-aggregated.csv\", tables[4])\n",
    "    print(\"    Data Loading Complete\\n\")\n",
    "\n",
    "    con.commit()\n",
    "    print(\"Database Created Successfully ...\")\n",
    "    # with open('../dataset/covid-19/mysql_database/dump.sql','w') as fp:\n",
    "    #     for line in con.iterdump():\n",
    "    #         fp.write('%s\\n' % line)\n",
    "\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_database_flag:\n",
    "    create_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_country_state_list():\n",
    "    con = sqlite3.connect(database_path)\n",
    "\n",
    "    cur = con.execute(\"SELECT DISTINCT Province_State, Country_Region FROM reference;\")\n",
    "    # cur = con.execute(\"SELECT count(*) FROM reference;\")\n",
    "\n",
    "    countries_only=[]\n",
    "    states_only=[]\n",
    "    state_country_dict={}\n",
    "    place_lower_to_normal={}\n",
    "\n",
    "    for row in cur:\n",
    "        countries_only.append(row[1].lower())\n",
    "        place_lower_to_normal[row[1].lower()]=row[1]\n",
    "\n",
    "        if row[0]:\n",
    "            states_only.append(row[0].lower())\n",
    "            state_country_dict[row[0].lower()]=row[1].lower()\n",
    "            place_lower_to_normal[row[0].lower()]=row[0]\n",
    "\n",
    "    con.close()\n",
    "    return countries_only, states_only, state_country_dict, place_lower_to_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con = sqlite3.connect(database_path)\n",
    "# cur = con.execute(\"SELECT DISTINCT Province_State, Country_Region FROM reference;\")\n",
    "# print(cur.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_only, states_only, state_country_dict, place_lower_to_normal=generate_country_state_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_places(places):\n",
    "    place_dict={\n",
    "        'no_match':[],\n",
    "        'states':[],\n",
    "        'countries':[]\n",
    "    }\n",
    "\n",
    "    for place in places:\n",
    "        place=place.lower()\n",
    "        if place in states_only:\n",
    "            place_dict['states'].append(place)\n",
    "        elif place in countries_only:\n",
    "            place_dict['countries'].append(place)\n",
    "        else:\n",
    "            place_dict['no_match'].append(place)\n",
    "    \n",
    "    return place_dict\n",
    "\n",
    "def get_places(entities):\n",
    "    places=[]\n",
    "    for ent in entities:\n",
    "        if ent.label_ == 'GPE':\n",
    "            places.append(ent.text)\n",
    "    \n",
    "    place_dict=identify_places(places)\n",
    "    \n",
    "    return place_dict\n",
    "\n",
    "def find_regex(s):\n",
    "    X = re.search('....-..-..', s)\n",
    "    if(X):\n",
    "        return X.group()\n",
    "    \n",
    "    X = re.search('....-..', s)\n",
    "    if(X):\n",
    "        return X.group()\n",
    "\n",
    "def process_interval(time):\n",
    "    # print(time)\n",
    "    if(time['begin']>time['end']):\n",
    "        time['begin'], time['end']=time['end'], time['begin']\n",
    "\n",
    "    start_time=time['begin']\n",
    "    YMD=start_time.split('-')\n",
    "    if(len(YMD)==2):\n",
    "        YMD.append('XX')\n",
    "\n",
    "    YMD[0]='2020'\n",
    "    YMD[1]=YMD[1] if YMD[1]!='XX' else '01'\n",
    "    YMD[2]=YMD[2] if YMD[2]!='XX' else '01'\n",
    "    # YMD.reverse()\n",
    "    begin='-'.join(YMD)\n",
    "\n",
    "    end_time=time['end']\n",
    "    YMD=end_time.split('-')\n",
    "    if(len(YMD)==2):\n",
    "        YMD.append('XX')\n",
    "\n",
    "    YMD[0]='2020'\n",
    "    YMD[1]=YMD[1] if YMD[1]!='XX' else '12'\n",
    "    YMD[2]=YMD[2] if YMD[2]!='XX' else '31'\n",
    "    # YMD.reverse()\n",
    "    end='-'.join(YMD)\n",
    "\n",
    "    time['begin']=begin\n",
    "    time['end']=end\n",
    "\n",
    "    return time\n",
    "\n",
    "def get_time_duration(query):\n",
    "    time={}\n",
    "    parsed=sutime.parse(query)\n",
    "\n",
    "    if len(parsed)==1:\n",
    "        item=parsed[0]\n",
    "\n",
    "        if item['type']=='DATE':\n",
    "            if item['value'] != 'PRESENT_REF':\n",
    "                time['begin']=item['value']\n",
    "                time['end']=item['value']\n",
    "\n",
    "        elif item['type']=='DURATION' and type(item['value'])==type(time):\n",
    "            time['begin']=item['value']['begin']\n",
    "            time['end']=item['value']['end']\n",
    "    \n",
    "    elif len(parsed)==2 and parsed[0]['type']=='DATE' and parsed[1]['type']=='DATE':\n",
    "        time['begin']=min(parsed[0]['value'], parsed[1]['value'])\n",
    "        time['end']=max(parsed[0]['value'], parsed[1]['value'])\n",
    "\n",
    "    # print(parsed)\n",
    "    # for item in parsed:\n",
    "    #     time.append(item['value'])\n",
    "\n",
    "    if len(time)==0:\n",
    "        time['begin']='XXXX-XX-XX'\n",
    "        time['end']='XXXX-XX-XX'\n",
    "    \n",
    "    \n",
    "    time['begin']=find_regex(time['begin'])\n",
    "    time['end']=find_regex(time['end'])\n",
    "    # print(time)\n",
    "    time=process_interval(time)\n",
    "    if(time['begin']>time['end']):\n",
    "        time['begin'], time['end']=time['end'], time['begin']\n",
    "        \n",
    "    return time\n",
    "\n",
    "def remove_unnecessary(query):\n",
    "    doc=nlp(query)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='GPE' or ent.label_=='DATE':\n",
    "            query=query.replace(ent.text,\"\")\n",
    "    # print(query)\n",
    "\n",
    "    query=query.lower()\n",
    "    doc=nlp(query)\n",
    "    query=' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    for word,base in reverse_base_word_dict.items():\n",
    "        # query=query.replace(word,base)\n",
    "        query = ' '.join(base if w == word else w for w in query.split())\n",
    "\n",
    "    doc=nlp(query)\n",
    "    for token in doc:\n",
    "        if token.is_stop==True or token.dep_=='prep' or token.dep_=='punct':\n",
    "            # query=query.replace(token.text,\"\")\n",
    "            query = ' '.join(\"\" if w == token.text else w for w in query.split())\n",
    "    \n",
    "    return query\n",
    "\n",
    "def get_case_and_function_type(query):\n",
    "    processed_query = remove_unnecessary(query)\n",
    "    # print(query)\n",
    "    # print(processed_query)\n",
    "\n",
    "    case_types = ['confirm','recover','death','increase rate','active']\n",
    "    function_types = ['maximum', 'minimum', 'average', 'sum']\n",
    "\n",
    "    final_case=\"\"\n",
    "    for case in case_types:\n",
    "        if(processed_query.find(case)>=0):\n",
    "            final_case = case\n",
    "            break\n",
    "    \n",
    "    if final_case==\"\":\n",
    "        final_case='confirm'\n",
    "\n",
    "    final_func=\"\"\n",
    "    for func in function_types:\n",
    "        if(processed_query.find(func)>=0):\n",
    "            final_func = func\n",
    "            break\n",
    "    \n",
    "    if final_func==\"\":\n",
    "        final_func='sum'\n",
    "    \n",
    "    return final_case, final_func\n",
    "\n",
    "def get_operation_type(query):\n",
    "    operation_type = ['state', 'country']\n",
    "\n",
    "    query=query.lower()\n",
    "    doc=nlp(query)\n",
    "    query=' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    for word,base in reverse_base_word_dict.items():\n",
    "        # query=query.replace(word,base)\n",
    "        query = ' '.join(base if w == word else w for w in query.split())\n",
    "    \n",
    "    doc=nlp(query)\n",
    "    for token in doc:\n",
    "        if token.dep_!='compound':\n",
    "            if token.text == 'state' or token.text == 'country':\n",
    "                return token.text\n",
    "\n",
    "    return 'cases'\n",
    "\n",
    "def parse_parameters(query):\n",
    "    doc=nlp(query)\n",
    "    entities=doc.ents\n",
    "\n",
    "    place = get_places(entities)\n",
    "    time_duration = get_time_duration(query)\n",
    "    case_type, function_type = get_case_and_function_type(query)\n",
    "    operation_type = get_operation_type(query)\n",
    "\n",
    "    parameters={\n",
    "        'query':query,\n",
    "        'Place':place,\n",
    "        'Time Duration': time_duration,\n",
    "        'Case Type': case_type,\n",
    "        'Function Type': function_type,\n",
    "        'Operation Type': operation_type\n",
    "        }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    # print(query)\n",
    "    parameters=parse_parameters(query)\n",
    "    # print_entities(query)\n",
    "    #print_tokens(sent)\n",
    "    # doc=nlp(query)\n",
    "    # print([chunk.text for chunk in doc.noun_chunks])\n",
    "    # print(parameters)\n",
    "    # print()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list=[]\n",
    "for q in queries:\n",
    "    parameter_list.append(process_query(q))\n",
    "\n",
    "with open(parsed_parameter_save_path, 'wb') as f:\n",
    "    pickle.dump((parameter_list,state_country_dict, place_lower_to_normal), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'query': 'State having minimum number of death ratio till between may and august.',\n",
       "  'Place': {'no_match': [], 'states': [], 'countries': []},\n",
       "  'Time Duration': {'begin': '2020-05-31', 'end': '2020-08-01'},\n",
       "  'Case Type': 'death',\n",
       "  'Function Type': 'minimum',\n",
       "  'Operation Type': 'state'},\n",
       " {'query': 'State having maximum number of active cases in the world?',\n",
       "  'Place': {'no_match': [], 'states': [], 'countries': []},\n",
       "  'Time Duration': {'begin': '2020-01-01', 'end': '2020-12-31'},\n",
       "  'Case Type': 'active',\n",
       "  'Function Type': 'maximum',\n",
       "  'Operation Type': 'state'},\n",
       " {'query': 'State having maximum number of active cases in Australia till now?',\n",
       "  'Place': {'no_match': [], 'states': [], 'countries': ['australia']},\n",
       "  'Time Duration': {'begin': '2020-01-01', 'end': '2020-12-31'},\n",
       "  'Case Type': 'active',\n",
       "  'Function Type': 'maximum',\n",
       "  'Operation Type': 'state'},\n",
       " {'query': 'maximum number of Active cases in state Alabama?',\n",
       "  'Place': {'no_match': [], 'states': ['alabama'], 'countries': []},\n",
       "  'Time Duration': {'begin': '2020-01-01', 'end': '2020-12-31'},\n",
       "  'Case Type': 'active',\n",
       "  'Function Type': 'maximum',\n",
       "  'Operation Type': 'cases'},\n",
       " {'query': 'average number of Death cases in state Mississippi?',\n",
       "  'Place': {'no_match': [], 'states': ['mississippi'], 'countries': []},\n",
       "  'Time Duration': {'begin': '2020-01-01', 'end': '2020-12-31'},\n",
       "  'Case Type': 'death',\n",
       "  'Function Type': 'average',\n",
       "  'Operation Type': 'cases'}]"
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "source": [
    "parameter_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}