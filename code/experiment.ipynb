{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b25663beab90ae83024dffde40993f1001b9a8604896fa4c67256a04dde7dc3b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -c conda-forge spacy\n",
    "# ! python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import nltk\n",
    "from itertools import product\n",
    "import csv, sqlite3\n",
    "from spacy.lookups import Lookups\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"What are total death cases in US on 27-09-2020?\"\n",
    "# sentence = \"number of cities with total cases greater than 1000?\"\n",
    "\n",
    "def print_entities(sentence):\n",
    "\tdoc = nlp(sentence)\n",
    "\tprint(\"----> Entities:\")\n",
    "\tfor ent in doc.ents: \n",
    "\t\tprint(\"-------->\",ent.text, ent.start_char, ent.end_char, ent.label_) \n",
    "\n",
    "def print_tokens(sentence):\n",
    "\tdoc = nlp(sentence) \n",
    "\tprint(\"----> Tokens:\")\n",
    "\tfor token in doc: \n",
    "\t\t# if(token.dep_ == \"nsubj\"):\n",
    "\t\tprint(\"-------->\", token.text, token.pos_, token.dep_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries=[]\n",
    "with open(\"../possible-questions.txt\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line[:-1]\n",
    "        if(line):\n",
    "            queries.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    # print(query)\n",
    "    print_entities(query)\n",
    "    print_tokens(query)\n",
    "    doc=nlp(query)\n",
    "    print([chunk.text for chunk in doc.noun_chunks])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp('found')\n",
    "query=' '.join([token.lemma_ for token in doc])\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords=['case', 'find', 'covid', 'coronavirus', 'covid-19', 'covid19', 'world']\n",
    "for word in additional_stopwords:\n",
    "        nlp.vocab[word].is_stop=True\n",
    "\n",
    "assign_base_words={\n",
    "    'recover' : ['recover','recovery','cure','heal'],\n",
    "    'death' : ['death','fatality','fatal','demise','decease','die','expire'],\n",
    "    'confirm': ['confirm']\n",
    "}\n",
    "\n",
    "reverse_base_word_dict={}\n",
    "for base, l in assign_base_words.items():\n",
    "    for item in l:\n",
    "        reverse_base_word_dict[item]=base\n",
    "\n",
    "# table = nlp.vocab.lookups.get_table(\"lemma_lookup\")\n",
    "# for base, l in assign_base_words.items():\n",
    "#     for item in l:\n",
    "#         table[item]=base\n",
    "\n",
    "# doc=nlp(\"recovery\")\n",
    "# print(doc[0].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(query):\n",
    "    doc=nlp(query)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='GPE' or ent.label_=='DATE':\n",
    "            query=query.replace(ent.text,\"\")\n",
    "    print(query)\n",
    "\n",
    "    query=query.lower()\n",
    "    doc=nlp(query)\n",
    "    query=' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    for word,base in reverse_base_word_dict.items():\n",
    "        query=query.replace(word,base)\n",
    "\n",
    "    doc=nlp(query)\n",
    "    for token in doc:\n",
    "        if token.is_stop==True or token.dep_=='prep' or token.dep_=='punct':\n",
    "            query=query.replace(token.text,\"\")\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    print(query)\n",
    "    process_query(query)\n",
    "    # processed_query = remove_unnecessary(query)\n",
    "    # print(processed_query)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"most\"): \n",
    "    print(syn.name())\n",
    "    print(syn.lemmas())\n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name())\n",
    "print (set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet \n",
    "\n",
    "def caseTypeProbability(sent):\n",
    "    docs=nlp(sent)\n",
    "    case_type=[\"death\", \"recovery\", \"active\", \"confirm\"]\n",
    "\n",
    "    similarity=0.0\n",
    "    type=\"total\"\n",
    "    word=\"\"\n",
    "\n",
    "    # for case in case_type:\n",
    "    #     tempdoc=nlp(case)\n",
    "    #     tempdoc=tempdoc[0]\n",
    "        \n",
    "    #     for doc in docs:\n",
    "    #         temp=doc.similarity(tempdoc)\n",
    "\n",
    "    #         if similarity<temp:\n",
    "    #             similarity=temp\n",
    "    #             type=case\n",
    "    #             word=doc\n",
    "    \n",
    "    for case in case_type:\n",
    "        w1=wordnet.synsets(case)\n",
    "        # print(w1)\n",
    "        for doc in docs:\n",
    "            w2=wordnet.synsets(str(doc.text))\n",
    "            # print(w2)\n",
    "            # temp=w1.similarity(w2)\n",
    "\n",
    "            for i,j in list(product(*[w1,w2])):\n",
    "                # print(i,j)\n",
    "                temp = i.wup_similarity(j) # Wu-Palmer Similarity\n",
    "                # maxscore = score if maxscore < score else maxscore\n",
    "                if temp and similarity<temp:\n",
    "                    similarity=temp\n",
    "                    type=case\n",
    "                    word=doc\n",
    "    \n",
    "    return type, word, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    print(query)\n",
    "    print(caseTypeProbability(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.kb import KnowledgeBase\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
    "\n",
    "# adding entities\n",
    "# kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
    "# kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
    "# kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
    "\n",
    "# # adding aliases\n",
    "# kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
    "\n",
    "print(\"Number of entities in KB:\",kb.get_size_entities()) # 3\n",
    "print(\"Number of aliases in KB:\", kb.get_size_aliases()) # 2\n",
    "\n",
    "candidates = kb.get_candidates(\"Douglas\")\n",
    "for c in candidates:\n",
    "    print(\" \", c.entity_, c.prior_prob, c.entity_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_table(path, csv_name, table_name):\n",
    "    with open(dataset_path+csv_name,'r') as fin:\n",
    "        dr = csv.DictReader(fin)\n",
    "        to_db=[tuple(i.values()) for i in dr]\n",
    "    \n",
    "    count=len(dr.fieldnames)\n",
    "    bindings=\"?, \"*count\n",
    "\n",
    "    cur.executemany(\"INSERT INTO \"+table_name+\" VALUES (\"+bindings[:-2]+\");\", to_db)\n",
    "\n",
    "# def csv_to_table(path, csv_name, table_name):\n",
    "#     with open(dataset_path+csv_name,'r') as fin:\n",
    "#         dr = csv.DictReader(fin)\n",
    "#         to_db=[tuple(i.values()) for i in dr]\n",
    "    \n",
    "#     count=len(dr.fieldnames)\n",
    "#     bindings=\"?, \"*count\n",
    "\n",
    "#     cur.executemany(\"INSERT INTO \"+table_name+\" VALUES (\"+bindings[:-2]+\");\", to_db[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# con = sqlite3.connect(r\"D:\\My Study Folder\\Others\\NLP-Search-Engine-COVID-19-Dataset\\dataset\\covid-19\\mysql_database\\covid19.db\")\n",
    "# cur = con.cursor()\n",
    "\n",
    "# dataset_path=\"../dataset/covid-19/required_only/\"\n",
    "\n",
    "# tables=[(\"worldwide_aggregate\"),(\"reference\"),(\"timeseries\"),(\"us\")]\n",
    "# for table in tables:\n",
    "#     cur.execute(\"DROP TABLE IF EXISTS \"+table+\";\")\n",
    "\n",
    "# cur.execute(\"create table worldwide_aggregate(Date Date NOT NULL, Confirmed BIGINT NOT NULL, Recovered BIGINT NOT NULL, Deaths BIGINT NOT NULL, Increase_rate FLOAT default NULL, PRIMARY KEY (Date));\")\n",
    "\n",
    "# csv_to_table(dataset_path,\"worldwide-aggregate.csv\", tables[0])\n",
    "\n",
    "# cur.execute(\"create table us(Date Date NOT NULL, Admin2 VARCHAR(100) NOT NULL, Province_State VARCHAR(100) NOT NULL, Confirmed BIGINT NOT NULL, Deaths BIGINT NOT NULL, Country_Region VARCHAR(100) NOT NULL, PRIMARY KEY (Date, Admin2, Province_State));\")\n",
    "\n",
    "# csv_to_table(dataset_path,\"us_simplified.csv\", tables[3])\n",
    "\n",
    "# cur.execute(\"create table reference(UID INT NOT NULL, iso2 VARCHAR(20), iso3 VARCHAR(20), code3 INT, FIPS INT, Admin2 VARCHAR(100) NOT NULL, Province_State VARCHAR(100) NOT NULL, Country_Region VARCHAR(100) NOT NULL, Lat FLOAT NOT NULL, Long_ FLOAT NOT NULL, Combined_Key VARCHAR(100), Popolation BIGINT NOT NULL, PRIMARY KEY (UID));\")\n",
    "\n",
    "# csv_to_table(dataset_path,\"reference.csv\", tables[1])\n",
    "\n",
    "# cur.execute(\"create table timeseries(Date Date NOT NULL, Country_Region VARCHAR(100) NOT NULL, Province_State VARCHAR(100), Confirmed BIGINT NOT NULL, Recovered BIGINT NOT NULL, Deaths BIGINT NOT NULL, PRIMARY KEY (Date, Country_Region, Province_State));\")\n",
    "\n",
    "# csv_to_table(dataset_path,\"time-series-19-covid-combined.csv\", tables[2])\n",
    "\n",
    "# con.commit()\n",
    "\n",
    "# with open('../dataset/covid-19/mysql_database/dump.sql','w') as fp:\n",
    "#     for line in con.iterdump():\n",
    "#         fp.write('%s\\n' % line)\n",
    "\n",
    "# con.close()\n",
    "\n",
    "# con = sqlite3.connect(r\"D:\\My Study Folder\\Others\\NLP-Search-Engine-COVID-19-Dataset\\dataset\\covid-19\\mysql_database\\covid19-small.db\")\n",
    "# cur = con.cursor()\n",
    "\n",
    "# dataset_path=\"../dataset/covid-19/required_only/\"\n",
    "\n",
    "# tables=[(\"worldwide_aggregate\"),(\"reference\"),(\"timeseries\"),(\"us\")]\n",
    "# for table in tables:\n",
    "#     cur.execute(\"DROP TABLE IF EXISTS \"+table+\";\")\n",
    "\n",
    "# cur.execute(\"create table worldwide_aggregate(Date Date NOT NULL, Confirmed BIGINT NOT NULL, Recovered BIGINT NOT NULL, Deaths BIGINT NOT NULL, Increase_rate FLOAT default NULL, PRIMARY KEY (Date));\")\n",
    "\n",
    "# csv_to_table(dataset_path,\"worldwide-aggregate.csv\", tables[0])\n",
    "\n",
    "# cur.execute(\"create table us(Date Date NOT NULL, Admin2 VARCHAR(100) NOT NULL, Province_State VARCHAR(100) NOT NULL, Confirmed BIGINT NOT NULL, Deaths BIGINT NOT NULL, Country_Region VARCHAR(100) NOT NULL, PRIMARY KEY (Date, Admin2, Province_State));\")\n",
    "\n",
    "# csv_to_table(dataset_path,\"us_simplified.csv\", tables[3])\n",
    "\n",
    "# cur.execute(\"create table reference(UID INT NOT NULL, iso2 VARCHAR(20), iso3 VARCHAR(20), code3 INT, FIPS INT, Admin2 VARCHAR(100) NOT NULL, Province_State VARCHAR(100) NOT NULL, Country_Region VARCHAR(100) NOT NULL, Lat FLOAT NOT NULL, Long_ FLOAT NOT NULL, Combined_Key VARCHAR(100), Popolation BIGINT NOT NULL, PRIMARY KEY (UID));\")\n",
    "\n",
    "# csv_to_table(dataset_path,\"reference.csv\", tables[1])\n",
    "\n",
    "# cur.execute(\"create table timeseries(Date Date NOT NULL, Country_Region VARCHAR(100) NOT NULL, Province_State VARCHAR(100), Confirmed BIGINT NOT NULL, Recovered BIGINT NOT NULL, Deaths BIGINT NOT NULL, PRIMARY KEY (Date, Country_Region, Province_State));\")\n",
    "\n",
    "# csv_to_table(dataset_path,\"time-series-19-covid-combined.csv\", tables[2])\n",
    "\n",
    "# con.commit()\n",
    "\n",
    "# with open('../dataset/covid-19/mysql_database/dump-small.sql','w') as fp:\n",
    "#     for line in con.iterdump():\n",
    "#         fp.write('%s\\n' % line)\n",
    "\n",
    "# con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database_path=\"D:/My Study Folder/Others/NLP-Search-Engine-COVID-19-Dataset/dataset/covid-19/mysql_database/\"\n",
    "\n",
    "# with open(database_path+\"dump.sql\", 'r') as f:\n",
    "#     lines=f.read()\n",
    "#     lines=lines.replace('\"','')\n",
    "#     # print(lines[:1000])\n",
    "\n",
    "#     tables=[(\"worldwide_aggregate\"),(\"reference\"),(\"timeseries\"),(\"us\")]\n",
    "\n",
    "#     for table in tables:\n",
    "#         lines=lines.replace(table,\"`\"+table+\"`\")\n",
    "    \n",
    "#     with open(database_path+\"covid19-schema.sql\", 'w') as out:\n",
    "#         out.write(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlite3mysql -f \"D:/My Study Folder/Others/NLP-Search-Engine-COVID-19-Dataset/dataset/covid-19/mysql_database/covid19.db\" -u \"root\" -d \"covid19\"\n",
    "# sqlite3mysql -f \"D:/My Study Folder/Others/NLP-Search-Engine-COVID-19-Dataset/dataset/covid-19/mysql_database/covid19-small.db\" -u \"root\" -d \"covid19_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sutime import SUTime\n",
    "\n",
    "sutime = SUTime(mark_time_ranges=True, include_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in queries:\n",
    "    # parsed=json.dumps(sutime.parse(q), sort_keys=True, indent=4)\n",
    "    parsed=sutime.parse(q)\n",
    "    # print(parsed)\n",
    "    print(q)\n",
    "    for item in parsed:\n",
    "        print(\"--> \",item)\n",
    "    print()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"Which country saw highest number of death in the month of April?\"\n",
    "# q=\"State having maximum number of active cases in USA till now?\"\n",
    "parsed=sutime.parse(q)\n",
    "# print(parsed)\n",
    "print(q)\n",
    "for item in parsed:\n",
    "    print(\"--> \",item)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "for q in queries:\n",
    "    doc=nlp(q)\n",
    "    print(q)\n",
    "    # print([token.text for token in doc])\n",
    "    print([(token.text, token.dep_) for token in doc])\n",
    "    displacy.render(doc, style=\"dep\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install geonamescache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geonamescache\n",
    "\n",
    "gc = geonamescache.GeonamesCache()\n",
    "countries = gc.get_countries_by_names()\n",
    "# print countries dictionary\n",
    "countries=list(countries.keys())\n",
    "print(len(countries))\n",
    "\n",
    "cities=[]\n",
    "for country in countries:\n",
    "    city = list(gc.get_cities_by_names(country))\n",
    "\n",
    "# print countries dictionary\n",
    "print(len(list(countries.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geograpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(r\"D:\\My Study Folder\\Others\\NLP-Search-Engine-COVID-19-Dataset\\dataset\\covid-19\\mysql_database\\covid19.db\")\n",
    "# cur = con.cursor()\n",
    "\n",
    "# cur = con.execute(\"SELECT DISTINCT Country_Region FROM reference;\")\n",
    "# cur = con.execute(\"SELECT count(*) FROM reference;\")\n",
    "cur = con.execute(\"SELECT Confirmed, Date FROM worldwide_aggregate;\")\n",
    "\n",
    "for row in cur:\n",
    "    print(row)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "parsed_parameter_save_path='../dataset/covid-19/parsed_parameters.pickle'\n",
    "with open(parsed_parameter_save_path, 'rb') as f:\n",
    "    temp=pickle.load(f)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "query = \"which place has the highest number of cases in US?\"\n",
    "\n",
    "assign_base_words={\n",
    "    'recover' : ['recover','recovery','cure','heal'],\n",
    "    'death' : ['death','fatality','fatal','demise','decease','die','expire'],\n",
    "    'confirm': ['confirm'],\n",
    "    'active' : ['active', 'live'],\n",
    "    'maximum' : ['maximum', 'high', 'max', 'maximal', 'most'],\n",
    "    'minimum' : ['minimum', 'low', 'least', 'min'],\n",
    "    'average' : ['average', 'avg', 'normally', 'usually', 'generally'],\n",
    "    'state' : ['state', 'province'],\n",
    "    'country' : ['country', 'region', 'nation', 'place']\n",
    "}\n",
    "\n",
    "def get_reverse_dict(assign_base_words):\n",
    "    reverse_base_word_dict={}\n",
    "    for base, l in assign_base_words.items():\n",
    "        for item in l:\n",
    "            doc = nlp(item)\n",
    "            item = doc[0].lemma_\n",
    "            reverse_base_word_dict[item]=base\n",
    "\n",
    "    return reverse_base_word_dict\n",
    "for word,base in reverse_base_word_dict.items():\n",
    "    query=query.replace(word,base)\n",
    "re.sub(r'\\bold\\b', 'new', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(r\"D:\\My Study Folder\\Others\\NLP-Search-Engine-COVID-19-Dataset\\dataset\\covid-19\\mysql_database\\covid19.db\")\n",
    "# cur = con.cursor()\n",
    "\n",
    "# cur = con.execute(\"SELECT Province_State FROM (SELECT Province_State, (MAX(sum)-MIN(sum)) as cases FROM (SELECT Date, Province_State, SUM(Confirmed) as sum FROM us WHERE Date BETWEEN '2020-04-01' AND '2020-04-31' GROUP BY Date, Province_State) GROUP BY Province_State) WHERE cases = (SELECT MAX(cases) from (SELECT (MAX(sum)-MIN(sum)) as cases FROM (SELECT Date, Province_State, SUM(Confirmed) as sum FROM us WHERE Date BETWEEN '2020-04-01' AND '2020-04-31' GROUP BY Date, Province_State) GROUP BY Province_State));\")\n",
    "\n",
    "Which state of US was worst affected?\n",
    "\n",
    "cur = con.execute(\"SELECT Province_State, (MAX(sum)-MIN(sum)) as cases FROM (SELECT Date, Province_State, SUM(Confirmed) as sum FROM us WHERE Date BETWEEN '2020-04-01' AND '2020-04-31' GROUP BY Date, Province_State) GROUP BY Province_State;\")\n",
    "\n",
    "\n",
    "for row in cur:\n",
    "    print(row)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}